{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import skimage.io as io\n",
    "import skimage.transform as transform\n",
    "import torchvision\n",
    "import clip\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import pickle\n",
    "import json\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "from transformers import set_seed, GPT2Config, GPT2Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "def isEglish(s):\n",
    "    return s.isascii()\n",
    "\n",
    "def punc(s):\n",
    "    for c in string.punctuation:\n",
    "        s=s.replace(c,\"\")\n",
    "    return s.lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19755 captions loaded from json \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 19755/19755 [04:32<00:00, 72.49it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "9949 embeddings saved \n",
      "6279 captions loaded from json \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6279/6279 [01:26<00:00, 72.71it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "3144 embeddings saved \n",
      "6761 captions loaded from json \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 6761/6761 [01:32<00:00, 73.14it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "3370 embeddings saved \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def preprocess_pathvqa(split, out_path):\n",
    "    device = torch.device('cuda:0')\n",
    "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "    data =  pd.read_pickle('/media/tjvsonsbeek/Data1/vqa_datasets/pathvqa/pathVQAprocessed/split/qas/{}/{}_qa.pkl'.format(split,split))\n",
    "    print(\"%0d captions loaded from json \" % len(data))\n",
    "    all_img_prefixes = []\n",
    "    all_txt_prefixes = []\n",
    "    img_ids = []\n",
    "    img_paths = []\n",
    "    all_questions = []\n",
    "    all_answers = []\n",
    "    compact_dict = {} \n",
    "    for i in tqdm(range(len(data))):\n",
    "        d = data[i]\n",
    "        if d['answer']!=\"yes\" and d['answer']!=\"no\":\n",
    "            img_id = d[\"image\"]\n",
    "            filename = \"/media/tjvsonsbeek/Data1/vqa_datasets/pathvqa/pathVQAprocessed/split/images/{}/{}.jpg\".format(split,img_id)\n",
    "            with torch.no_grad():\n",
    "                prefix_i = clip_model.encode_image(preprocess(Image.open(filename)).unsqueeze(0).to(device)).cpu()\n",
    "                prefix_t = clip_model.encode_text(clip.tokenize(d['question']).to(device)).cpu()\n",
    "                \n",
    "            if img_id not in compact_dict.keys():\n",
    "                compact_dict[img_id] = [[d['question']],[d['answer']],[prefix_t],prefix_i,filename]\n",
    "            else:\n",
    "                compact_dict[img_id][0].append(d['question'])\n",
    "                compact_dict[img_id][1].append(d['answer'])\n",
    "                compact_dict[img_id][2].append(prefix_t)\n",
    "    for img_id, imgs in enumerate(compact_dict.keys()):\n",
    "        all_img_prefixes.append(compact_dict[imgs][3])\n",
    "        for q in range(len(compact_dict[imgs][0])):\n",
    "            all_txt_prefixes.append(compact_dict[imgs][2][q])\n",
    "            all_questions.append(compact_dict[imgs][0][q])\n",
    "            all_answers.append(compact_dict[imgs][1][q])\n",
    "            img_ids.append(img_id)\n",
    "            img_paths.append(compact_dict[imgs][4])\n",
    "\n",
    "    all_data = {\"img_prefix\": torch.cat(all_img_prefixes, dim=0), \"img_ids\": img_ids, \"questions\": all_questions,'answers': all_answers, 'txt_prefix': torch.cat(all_txt_prefixes, dim=0),'img_path': img_paths}\n",
    "\n",
    "    with open(out_path, 'wb') as f:\n",
    "        pickle.dump(all_data,f)\n",
    "    print('Done')\n",
    "    print(\"%0d embeddings saved \" % len(all_txt_prefixes))\n",
    "for split in ['train','val','test']:\n",
    "    out_path = \"/media/tjvsonsbeek/Data1/vqa_datasets/pathvqa/oa_{}.pkl\".format(split)\n",
    "    preprocess_pathvqa(split,out_path)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 21)\n",
      "1056\n",
      "15189\n"
     ]
    }
   ],
   "source": [
    "def update_classes(pkl_train, pkl_val, pkl_test):\n",
    "    with open(pkl_train, 'rb') as f:\n",
    "            data_train = pickle.load(f)\n",
    "    with open(pkl_val, 'rb') as f:\n",
    "            data_val = pickle.load(f)\n",
    "    with open(pkl_test, 'rb') as f:\n",
    "            data_test = pickle.load(f)\n",
    "    \n",
    "    cur_id = 0\n",
    "    class_names_list = []\n",
    "    class_ids_list = [[],[],[]]\n",
    "\n",
    "\n",
    "\n",
    "    for i, data in enumerate([data_train,data_val,data_test]):\n",
    "        \n",
    "        for answer in data['answers']:\n",
    "            if answer not in class_names_list:\n",
    "                class_names_list.append(answer)\n",
    "                class_ids_list[i].append(cur_id)\n",
    "                cur_id+=1\n",
    "            else:\n",
    "                class_ids_list[i].append(class_names_list.index(answer))\n",
    "    q_lens = []\n",
    "    a_lens = []\n",
    "    for question in data_train['questions']:\n",
    "        q_lens.append(len(tokenizer.encode(question)))\n",
    "    for answer in data_train['answers']:\n",
    "        a_lens.append(len(tokenizer.encode(str(answer))))\n",
    "    print((int(np.mean(q_lens)+2*np.std(q_lens)),int(np.mean(a_lens)+2*np.std(a_lens))))\n",
    "    \n",
    "\n",
    "    \n",
    "    data_train['class_ids'] = class_ids_list[0]\n",
    "    data_val['class_ids'] = class_ids_list[1]\n",
    "    data_test['class_ids'] = class_ids_list[2]\n",
    "    \n",
    "    data_train['class_names'] = class_names_list\n",
    "    data_val['class_names'] = class_names_list\n",
    "    data_test['class_names'] = class_names_list\n",
    "    \n",
    "    data_train['max_classes']=len(class_names_list)\n",
    "    data_val['max_classes']=len(class_names_list)\n",
    "    data_test['max_classes']=len(class_names_list)\n",
    "    print(len(class_names_list))\n",
    "    print(len(data_train['questions']))\n",
    "    data_train['max_seqs_len']=(int(np.mean(q_lens)+2*np.std(q_lens)),int(np.mean(a_lens)+2*np.std(a_lens)))\n",
    "    data_val['max_seqs_len']=(int(np.mean(q_lens)+2*np.std(q_lens)),int(np.mean(a_lens)+2*np.std(a_lens)))\n",
    "    data_test['max_seqs_len']=(int(np.mean(q_lens)+2*np.std(q_lens)),int(np.mean(a_lens)+2*np.std(a_lens)))\n",
    "    \n",
    "    with open(pkl_train, 'wb') as f:\n",
    "        pickle.dump(data_train,f)\n",
    "    with open(pkl_val, 'wb') as f:\n",
    "        pickle.dump(data_val,f)\n",
    "    with open(pkl_test, 'wb') as f:\n",
    "        pickle.dump(data_test,f)\n",
    "# update_classes(\"/media/tjvsonsbeek/Data1/vqa_datasets/pathvqa/oa_{}.pkl\".format('train'),\n",
    "#                \"/media/tjvsonsbeek/Data1/vqa_datasets/pathvqa/oa_{}.pkl\".format('val'),\n",
    "#                \"/media/tjvsonsbeek/Data1/vqa_datasets/pathvqa/oa_{}.pkl\".format('test'))\n",
    "# update_classes('/media/tjvsonsbeek/Data1/vqa_datasets/radvqa/train.pkl',\n",
    "#                '/media/tjvsonsbeek/Data1/vqa_datasets/radvqa/val.pkl',\n",
    "#                '/media/tjvsonsbeek/Data1/vqa_datasets/radvqa/test.pkl')\n",
    "# update_classes('/media/tjvsonsbeek/Data1/vqa_datasets/slake/train.pkl',\n",
    "#                '/media/tjvsonsbeek/Data1/vqa_datasets/slake/val.pkl',\n",
    "#                '/media/tjvsonsbeek/Data1/vqa_datasets/slake/test.pkl')\n",
    "update_classes('/media/tjvsonsbeek/Data1/vqa_datasets/ovqa/train.pkl',\n",
    "               '/media/tjvsonsbeek/Data1/vqa_datasets/ovqa/val.pkl',\n",
    "               '/media/tjvsonsbeek/Data1/vqa_datasets/ovqa/test.pkl')\n",
    "# update_classes('/media/tjvsonsbeek/Data1/vqa_datasets/medvqa2019/train.pkl',\n",
    "#                '/media/tjvsonsbeek/Data1/vqa_datasets/medvqa2019/val.pkl',\n",
    "#                '/media/tjvsonsbeek/Data1/vqa_datasets/medvqa2019/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "answer_tokens = []\n",
    "question2img = []\n",
    "max_seq_len = 0\n",
    "for answer, img_id in zip(answers_raw,img_ids):\n",
    "    answer_tokens.append(torch.tensor(self.tokenizer.encode(str(answer)), dtype=torch.int64))\n",
    "    question2img.append(img_id)\n",
    "    max_seq_len = max(max_seq_len, self.answer_tokens[-1].shape[0])\n",
    "\n",
    "# self.max_seq_len = max_seq_len\n",
    "with open(f\"{data_path[:-4]}_tokens.pkl\", 'wb') as f:\n",
    "    pickle.dump([self.answer_tokens, self.question2img, max_seq_len], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15216 captions loaded from json \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/15216 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 43\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mfor\u001b[39;00m split \u001b[39min\u001b[39;00m [\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m]:\n\u001b[1;32m     42\u001b[0m     out_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m/media/tjvsonsbeek/Data1/vqa_datasets/ovqa/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.pkl\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(split)\n\u001b[0;32m---> 43\u001b[0m     preprocess_ovqa(split,out_path)\n",
      "Cell \u001b[0;32mIn[6], line 22\u001b[0m, in \u001b[0;36mpreprocess_ovqa\u001b[0;34m(split, out_path)\u001b[0m\n\u001b[1;32m     20\u001b[0m     prefix_i \u001b[39m=\u001b[39m clip_model\u001b[39m.\u001b[39mencode_image(preprocess(Image\u001b[39m.\u001b[39mopen(filename))\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\u001b[39m.\u001b[39mto(device))\u001b[39m.\u001b[39mcpu()          \n\u001b[1;32m     21\u001b[0m \u001b[39mif\u001b[39;00m img_id \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m compact_dict\u001b[39m.\u001b[39mkeys():\n\u001b[0;32m---> 22\u001b[0m     compact_dict[img_id] \u001b[39m=\u001b[39m [[punc(d[\u001b[39m'\u001b[39;49m\u001b[39mquestion\u001b[39;49m\u001b[39m'\u001b[39;49m])],[punc(d[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m])],prefix_i,filename]\n\u001b[1;32m     23\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     compact_dict[img_id][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mappend(punc(d[\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m]))\n",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m, in \u001b[0;36mpunc\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpunc\u001b[39m(s):\n\u001b[0;32m---> 21\u001b[0m     \u001b[39mfor\u001b[39;00m c \u001b[39min\u001b[39;00m string\u001b[39m.\u001b[39mpunctuation:\n\u001b[1;32m     22\u001b[0m         s\u001b[39m=\u001b[39ms\u001b[39m.\u001b[39mreplace(c,\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     23\u001b[0m     \u001b[39mreturn\u001b[39;00m s\u001b[39m.\u001b[39mlower()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'string' is not defined"
     ]
    }
   ],
   "source": [
    "def preprocess_ovqa(split, out_path):\n",
    "    device = torch.device('cuda:0')\n",
    "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "    with open('/media/tjvsonsbeek/Data1/vqa_datasets/ovqa/{}set.json'.format(split)) as f:\n",
    "        data =  json.load(f)\n",
    "    print(\"%0d captions loaded from json \" % len(data))\n",
    "    all_img_prefixes = []\n",
    "    all_txt_prefixes = []\n",
    "    img_ids = []\n",
    "    img_paths = []\n",
    "    all_questions = []\n",
    "    all_answers = []\n",
    "    compact_dict = {} \n",
    "    for i in tqdm(range(len(data))):\n",
    "        d = data[i]\n",
    "        if isEglish(d['answer']) and isEglish(d['question']):\n",
    "            img_id = d[\"image_name\"][:-4]\n",
    "            filename = \"/media/tjvsonsbeek/Data1/vqa_datasets/ovqa/img/\"+d['image_name']\n",
    "            with torch.no_grad():\n",
    "                prefix_i = clip_model.encode_image(preprocess(Image.open(filename)).unsqueeze(0).to(device)).cpu()          \n",
    "            if img_id not in compact_dict.keys():\n",
    "                compact_dict[img_id] = [[punc(d['question'])],[punc(d['answer'])],prefix_i,filename]\n",
    "            else:\n",
    "                compact_dict[img_id][0].append(punc(d['question']))\n",
    "                compact_dict[img_id][1].append(punc(d['answer']))\n",
    "    for img_id, imgs in enumerate(compact_dict.keys()):\n",
    "        all_img_prefixes.append(compact_dict[imgs][2])\n",
    "        for q in range(len(compact_dict[imgs][0])):\n",
    "            all_questions.append(compact_dict[imgs][0][q])\n",
    "            all_answers.append(compact_dict[imgs][1][q])\n",
    "            img_ids.append(img_id)\n",
    "            img_paths.append(compact_dict[imgs][2])\n",
    "    check=torch.cat(all_img_prefixes, dim=0)\n",
    "\n",
    "    all_data = {\"img_prefix\": torch.cat(all_img_prefixes, dim=0), \"img_ids\": img_ids, \"questions\": all_questions,'answers': all_answers,'img_path': img_paths}\n",
    "\n",
    "    with open(out_path, 'wb') as f:\n",
    "        pickle.dump(all_data,f)\n",
    "    print('Done')\n",
    "    print(\"%0d embeddings saved \" % len(all_questions))\n",
    "for split in ['train','test','val']:\n",
    "    out_path = \"/media/tjvsonsbeek/Data1/vqa_datasets/ovqa/{}.pkl\".format(split)\n",
    "    preprocess_ovqa(split,out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9835 captions loaded from json \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9835/9835 [01:23<00:00, 117.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "4919 embeddings saved \n",
      "2094 captions loaded from json \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2094/2094 [00:18<00:00, 114.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "1061 embeddings saved \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "def preprocess_slake(split, out_path):\n",
    "    device = torch.device('cuda:0')\n",
    "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "    with open('/media/tjvsonsbeek/Data1/vqa_datasets/slake/Slake1.0/{}.json'.format(split)) as f:\n",
    "        data =  json.load(f)\n",
    "    print(\"%0d captions loaded from json \" % len(data))\n",
    "    all_img_prefixes = []\n",
    "    all_txt_prefixes = []\n",
    "    img_ids = []\n",
    "    img_paths = []\n",
    "    all_questions = []\n",
    "    all_answers = []\n",
    "    compact_dict = {} \n",
    "    for i in tqdm(range(len(data))):\n",
    "        d = data[i]\n",
    "        if isEglish(d['answer']) and isEglish(d['question']):\n",
    "            img_id = d[\"img_id\"]\n",
    "            filename = \"/media/tjvsonsbeek/Data1/vqa_datasets/slake/Slake1.0/imgs/\"+d['img_name']\n",
    "            with torch.no_grad():\n",
    "                prefix_i = clip_model.encode_image(preprocess(Image.open(filename)).unsqueeze(0).to(device)).cpu()          \n",
    "            if img_id not in compact_dict.keys():\n",
    "                compact_dict[img_id] = [[d['question']],[d['answer']],prefix_i,filename]\n",
    "            else:\n",
    "                compact_dict[img_id][0].append(d['question'])\n",
    "                compact_dict[img_id][1].append(d['answer'])\n",
    "    for img_id, imgs in enumerate(compact_dict.keys()):\n",
    "        all_img_prefixes.append(compact_dict[imgs][2])\n",
    "        for q in range(len(compact_dict[imgs][0])):\n",
    "            all_questions.append(compact_dict[imgs][0][q])\n",
    "            all_answers.append(compact_dict[imgs][1][q])\n",
    "            img_ids.append(img_id)\n",
    "            img_paths.append(compact_dict[imgs][2])\n",
    "    check=torch.cat(all_img_prefixes, dim=0)\n",
    "\n",
    "    all_data = {\"img_prefix\": torch.cat(all_img_prefixes, dim=0), \"img_ids\": img_ids, \"questions\": all_questions,'answers': all_answers,'img_path': img_paths}\n",
    "\n",
    "    with open(out_path, 'wb') as f:\n",
    "        pickle.dump(all_data,f)\n",
    "    print('Done')\n",
    "    print(\"%0d embeddings saved \" % len(all_questions))\n",
    "for split in ['train','test']:\n",
    "    out_path = \"/media/tjvsonsbeek/Data1/vqa_datasets/slake/{}.pkl\".format(split)\n",
    "    preprocess_slake(split,out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2248/2248 [00:59<00:00, 37.78it/s]\n",
      "313it [00:00, 356.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "1455 embeddings saved \n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda:0')\n",
    "clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "with open(\"/media/tjvsonsbeek/Data1/vqa_datasets/radvqa/radvqa/VQA_RAD Dataset Public.json\", 'r') as f:\n",
    "  data = json.load(f)\n",
    "compact_dict = {} \n",
    "compact_dict_test = {}\n",
    "for i in tqdm(range(len(data))):\n",
    "    d = data[i]\n",
    "    img_id = d[\"image_name\"]\n",
    "    filename = \"/media/tjvsonsbeek/Data1/vqa_datasets/radvqa/radvqa/VQA_RAD_Image_Folder/{}\".format(img_id)\n",
    "    with torch.no_grad():\n",
    "        prefix_i = clip_model.encode_image(preprocess(Image.open(filename)).unsqueeze(0).to(device)).cpu()\n",
    "        prefix_t = clip_model.encode_text(clip.tokenize(d['question']).to(device)).cpu()\n",
    "    if 'test' in d['phrase_type']:\n",
    "        if img_id not in compact_dict_test.keys():\n",
    "            compact_dict_test[img_id] = [[d['question']],[d['answer']],[prefix_t],prefix_i]\n",
    "        else:\n",
    "            compact_dict_test[img_id][0].append(d['question'])\n",
    "            compact_dict_test[img_id][1].append(d['answer'])\n",
    "            compact_dict_test[img_id][2].append(prefix_t)\n",
    "    else:\n",
    "        if img_id not in compact_dict.keys():\n",
    "            compact_dict[img_id] = [[d['question']],[d['answer']],[prefix_t],prefix_i]\n",
    "        else:\n",
    "            compact_dict[img_id][0].append(d['question'])\n",
    "            compact_dict[img_id][1].append(d['answer'])\n",
    "            compact_dict[img_id][2].append(prefix_t)\n",
    "#trainval\n",
    "all_img_prefixes = []\n",
    "for img_id, imgs in enumerate(compact_dict.keys()):\n",
    "    all_img_prefixes.append(compact_dict[imgs][3])\n",
    "n = len(all_img_prefixes)\n",
    "indices = random.sample(range(n), int(0.8 * n)) # generate random indices for the training set\n",
    "t_img_prefixes = [ all_img_prefixes[i] for i in indices ]\n",
    "v_img_prefixes = [ all_img_prefixes[i] for i in range(n) if i not in indices ]\n",
    "\n",
    "\n",
    "all_txt_prefixes = []\n",
    "img_ids = []\n",
    "all_questions = []\n",
    "all_answers = []\n",
    "\n",
    "val_all_txt_prefixes = []\n",
    "val_img_ids = []\n",
    "val_all_questions = []\n",
    "val_all_answers = []\n",
    "\n",
    "for img_id, imgs in tqdm(enumerate(compact_dict.keys())):\n",
    "    if np.any([np.all(np.equal(compact_dict[imgs][3].numpy(), tensor_i.numpy())) for tensor_i in t_img_prefixes]):\n",
    "        for q in range(len(compact_dict[imgs][0])):\n",
    "            all_txt_prefixes.append(compact_dict[imgs][2][q])\n",
    "            all_questions.append(compact_dict[imgs][0][q])\n",
    "            all_answers.append(compact_dict[imgs][1][q])\n",
    "            img_ids.append(img_id)\n",
    "    elif np.any([np.all(np.equal(compact_dict[imgs][3].numpy(), tensor_i.numpy())) for tensor_i in v_img_prefixes]):\n",
    "        for q in range(len(compact_dict[imgs][0])):\n",
    "            val_all_txt_prefixes.append(compact_dict[imgs][2][q])\n",
    "            val_all_questions.append(compact_dict[imgs][0][q])\n",
    "            val_all_answers.append(compact_dict[imgs][1][q])\n",
    "            val_img_ids.append(img_id)\n",
    "    else:\n",
    "        print('shouldnt happend')\n",
    "\n",
    "\n",
    "train_data = {\"img_prefix\": torch.cat(t_img_prefixes, dim=0), \"img_ids\": img_ids, \"questions\": all_questions,'answers': all_answers, 'txt_prefix': torch.cat(all_txt_prefixes, dim=0)}\n",
    "with open('/media/tjvsonsbeek/Data1/vqa_datasets/radvqa/train.pkl', 'wb') as f:\n",
    "    pickle.dump(train_data,f)\n",
    "\n",
    "val_data = {\"img_prefix\": torch.cat(v_img_prefixes, dim=0), \"img_ids\": val_img_ids, \"questions\": val_all_questions,'answers': val_all_answers, 'txt_prefix': torch.cat(val_all_txt_prefixes, dim=0)}\n",
    "with open('/media/tjvsonsbeek/Data1/vqa_datasets/radvqa/val.pkl', 'wb') as f:\n",
    "    pickle.dump(val_data,f)\n",
    "\n",
    "\n",
    "#test\n",
    "t_all_img_prefixes = []\n",
    "t_all_txt_prefixes = []\n",
    "t_img_ids = []\n",
    "t_all_questions = []\n",
    "t_all_answers = []\n",
    "for img_id, imgs in enumerate(compact_dict_test.keys()):\n",
    "    t_all_img_prefixes.append(compact_dict_test[imgs][3])\n",
    "    for q in range(len(compact_dict_test[imgs][0])):\n",
    "        t_all_txt_prefixes.append(compact_dict_test[imgs][2][q])\n",
    "        t_all_questions.append(compact_dict_test[imgs][0][q])\n",
    "        t_all_answers.append(compact_dict_test[imgs][1][q])\n",
    "        t_img_ids.append(img_id)\n",
    "\n",
    "test_data = {\"img_prefix\": torch.cat(all_img_prefixes, dim=0), \"img_ids\": img_ids, \"questions\": all_questions,'answers': all_answers, 'txt_prefix': torch.cat(all_txt_prefixes, dim=0)}\n",
    "with open('/media/tjvsonsbeek/Data1/vqa_datasets/radvqa/test.pkl', 'wb') as f:\n",
    "    pickle.dump(test_data,f)\n",
    "print('Done')\n",
    "print(\"%0d embeddings saved \" % len(all_txt_prefixes))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12792 captions loaded from json \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12792/12792 [06:09<00:00, 34.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "12792 embeddings saved \n"
     ]
    }
   ],
   "source": [
    "def preprocess_medvqa(split, out_path):\n",
    "    device = torch.device('cuda:0')\n",
    "    img_base_path = '/media/tjvsonsbeek/Data1/vqa_datasets/medvqa2019/Train_images/'\n",
    "    clip_model, preprocess = clip.load(\"ViT-B/32\", device=device, jit=False)\n",
    "    with open('/media/tjvsonsbeek/Data1/vqa_datasets/medvqa2019/All_QA_Pairs_train.txt') as f:\n",
    "        data = f.readlines()\n",
    "    # print(data.head())\n",
    "    # with open(\"/media/tjvsonsbeek/Data1/vqa_datasets/radvqa/radvqa/VQA_RAD_Dataset_Public.xlsx\") as f:\n",
    "    #    print(f)\n",
    "    print(\"%0d captions loaded from json \" % len(data))\n",
    "    all_img_prefixes = []\n",
    "    all_txt_prefixes = []\n",
    "    img_ids = []\n",
    "    all_questions = []\n",
    "    all_answers = []\n",
    "    compact_dict = {} \n",
    "    for i in tqdm(range(len(data))):\n",
    "        d = data[i].strip().split('|')\n",
    "        img_id = d[0]\n",
    "        \n",
    "        filename = img_base_path + img_id+'.jpg'\n",
    "        with torch.no_grad():\n",
    "            prefix_i = clip_model.encode_image(preprocess(Image.open(filename)).unsqueeze(0).to(device)).cpu()\n",
    "            prefix_t = clip_model.encode_text(clip.tokenize(d[1]).to(device)).cpu()\n",
    "            \n",
    "        if img_id not in compact_dict.keys():\n",
    "            compact_dict[img_id] = [[d[1]],[d[2]],[prefix_t],prefix_i]\n",
    "        else:\n",
    "            compact_dict[img_id][0].append(d[1])\n",
    "            compact_dict[img_id][1].append(d[2])\n",
    "            compact_dict[img_id][2].append(prefix_t)\n",
    "            \n",
    "\n",
    "\n",
    "    for img_id, imgs in enumerate(compact_dict.keys()):\n",
    "        all_img_prefixes.append(compact_dict[imgs][3])\n",
    "        for q in range(len(compact_dict[imgs][0])):\n",
    "            all_txt_prefixes.append(compact_dict[imgs][2][q])\n",
    "            all_questions.append(compact_dict[imgs][0][q])\n",
    "            all_answers.append(compact_dict[imgs][1][q])\n",
    "            img_ids.append(img_id)\n",
    "    all_data = {\"img_prefix\": torch.cat(all_img_prefixes, dim=0), \"img_ids\": img_ids, \"questions\": all_questions,'answers': all_answers, 'txt_prefix': torch.cat(all_txt_prefixes, dim=0)}    \n",
    "    with open(out_path, 'wb') as f:\n",
    "        pickle.dump(all_data,f)\n",
    "    print('Done')\n",
    "    print(\"%0d embeddings saved \" % len(all_txt_prefixes))\n",
    "for split in ['train','val','test']:\n",
    "    out_path = \"/media/tjvsonsbeek/Data1/vqa_datasets/medvqa2019/processed_{}.pkl\".format(split)\n",
    "    preprocess_pathvqa(split,out_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n",
      "6761 embeddings saved \n"
     ]
    }
   ],
   "source": [
    "with open(out_path, 'wb') as f:\n",
    "    pickle.dump({\"img_prefix\": torch.cat(all_img_prefixes, dim=0), \"img_ids\": img_ids, \"questions\": all_questions,'answers': all_answers, 'txt_prefix': torch.cat(all_txt_prefixes, dim=0)},f)\n",
    "print('Done')\n",
    "print(\"%0d embeddings saved \" % len(all_txt_prefixes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def expand_vqa_set(in_file, out_path):\n",
    "#     with open(in_file,'rb') as f:\n",
    "#         compact_dict = pickle.load(f)\n",
    "#     all_imgs = []\n",
    "#     all_questions = []\n",
    "#     all_answers = []\n",
    "#     for q_id, q in tqdm(enumerate(compact_dict['questions'])):\n",
    "#         for s_id, subquestions in enumerate(q):\n",
    "#             all_imgs.append(q_id)\n",
    "#             # print(type(compact_dict[imgs][2]))\n",
    "#             all_questions.append(compact_dict['questions'][s_id])\n",
    "#             all_answers.append(compact_dict['answers'][s_id])\n",
    "#     with open(out_path, 'wb') as f:\n",
    "#         pickle.dump({\"clip_embedding\": compact_dict['clip_embedding'], \"img_ids\": all_imgs, \"questions\": all_questions,'answers': all_answers},f)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2599it [00:00, 282760.84it/s]\n",
      "832it [00:00, 285967.46it/s]\n",
      "858it [00:00, 417726.39it/s]\n"
     ]
    }
   ],
   "source": [
    "expand_vqa_set(\"/media/tjvsonsbeek/Data1/vqa_datasets/pathvqa/processed_train.pkl\",\"/media/tjvsonsbeek/Data1/vqa_datasets/pathvqa/processed_long_train.pkl\")\n",
    "expand_vqa_set(\"/media/tjvsonsbeek/Data1/vqa_datasets/pathvqa/processed_val.pkl\",\"/media/tjvsonsbeek/Data1/vqa_datasets/pathvqa/processed_long_val.pkl\")\n",
    "expand_vqa_set(\"/media/tjvsonsbeek/Data1/vqa_datasets/pathvqa/processed_test.pkl\",\"/media/tjvsonsbeek/Data1/vqa_datasets/pathvqa/processed_long_test.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/media/tjvsonsbeek/Data1/vqa_datasets/radvqa/radvqa/VQA_RAD%20Dataset%20Public.json\"\n",
    "with open(data_path, 'rb') as f:\n",
    "    all_data = pickle.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/media/tjvsonsbeek/Data1/vqa_datasets/radvqa/radvqa/VQA_RAD Dataset Public.json\", 'r') as f:\n",
    "  data = json.load(f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'qid': '0',\n",
       " 'phrase_type': 'freeform',\n",
       " 'qid_linked_id': '03f451ca-de62-4617-9679-e836026a7642',\n",
       " 'image_case_url': 'https://medpix.nlm.nih.gov/case?id=48e1dd0e-8552-46ad-a354-5eb55be86de6',\n",
       " 'image_name': 'synpic54610.jpg',\n",
       " 'image_organ': 'HEAD',\n",
       " 'evaluation': 'not evaluated',\n",
       " 'question': 'Are regions of the brain infarcted?',\n",
       " 'question_rephrase': 'NULL',\n",
       " 'question_relation': 'NULL',\n",
       " 'question_frame': 'NULL',\n",
       " 'question_type': 'PRES',\n",
       " 'answer': 'Yes',\n",
       " 'answer_type': 'CLOSED'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cndmedvqa",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f1043dac98ebd3b9be7889d496fe6fee499649df15bb674be35d860f69682584"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
